{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25d98fb-9a1e-4f98-b3fd-10b8ae565583",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 准备基础环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751bc8af-7e6d-443c-9ee3-b4fd42575282",
   "metadata": {},
   "source": [
    "## 1.1 升级Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7794a87-f631-4925-bb2d-a8355225a097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade boto3\n",
    "# !pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fde6af-3414-478f-be27-6e28203c36c9",
   "metadata": {},
   "source": [
    "## 1.2 获取Runtime资源配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dfe8c4c-cb83-4306-a823-3b0eb119b47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28bdd62-9c7c-45e8-854d-721bc9d2d71b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-568765279027'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_default_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a16e22-ad1a-44b3-a174-b0f6a8a96c80",
   "metadata": {},
   "source": [
    "# 2. 准备微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff91a8e-44e6-47e3-8eb5-1a458476d28f",
   "metadata": {},
   "source": [
    "## 2.1 准备微调数据\n",
    "我们使用上一部分的医疗问答数据来做微调。之前我们把数据生成了文本文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44baf3b5-8eed-4388-8ffe-005d53a3fa92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "r = random.randint(0, 100)\n",
    "\n",
    "with open('./contents/med_qa.csv', encoding=\"utf-8\") as f:\n",
    "    med_qa_file = f.read()\n",
    "\n",
    "local_cache_path = Path(\"./finetune_data\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "train_file = open('./finetune_data/med_qa_train.json', 'w')\n",
    "test_file = open('./finetune_data/med_qa_test.json', 'w')\n",
    "\n",
    "datas = []\n",
    "for qa_text in med_qa_file.split('\\n\\n'):\n",
    "    if qa_text:\n",
    "        qa_text = qa_text.split('\\nA:')\n",
    "        q = qa_text[0].replace(\"Q:\", \"\")\n",
    "        a = qa_text[1]\n",
    "        line = {\"question\": q, \"answer\": a}\n",
    "        line = json.dumps(line, ensure_ascii=False) + \"\\n\"\n",
    "        if random.randint(0, 100) < 2:\n",
    "            test_file.write(line)\n",
    "        else:\n",
    "            train_file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12d0d0-e499-4625-b778-36309609c7bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "数据如下所示：\n",
    "```json\n",
    "{\n",
    "    \"question\": \"不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\",·\n",
    "    \"answer\": \"B超属于超声波经常检查是不好的而且也没有必要经常检查的一般怀孕两个月检查一下怀孕五个月检查一下快出生时在检查就可以还有就是不舒服检查就可以的\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f1e216e-d3fc-486f-a896-50f5ef162eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp finetune_data/med_qa_test.json s3://sagemaker-us-east-1-568765279027/llm/datasets/chatglm2/med_qa/med_qa_test.json\n",
      "cp finetune_data/med_qa_train.json s3://sagemaker-us-east-1-568765279027/llm/datasets/chatglm2/med_qa/med_qa_train.json\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "\n",
    "./s5cmd sync ./finetune_data/ s3://${sagemaker_default_bucket}/llm/datasets/chatglm2/med_qa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1790e2-5800-411d-8c6c-af26618670a7",
   "metadata": {},
   "source": [
    "## 2.3 下载ChatGLM2原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05fcfc-89a1-45fb-85ea-1681a1316a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3acd1f6f-6172-4326-8390-3f3a75361898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edb56ef29ab4e019e7244f212ecc28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"/home/ec2-user/SageMaker/models\")\n",
    "\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"THUDM/chatglm2-6b\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf01bf-4747-4300-8a7b-4d10ec65b2d6",
   "metadata": {},
   "source": [
    "将模型上传到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42c0ce80-883b-46bf-a642-e8f69a5fdaab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b\n"
     ]
    }
   ],
   "source": [
    "# upload to s3\n",
    "key_prefix=\"mt_models_uploaded/THUDM--chatglm2-6b\"\n",
    "model_name_or_path = sess.upload_data(path=model_download_path, key_prefix=key_prefix)\n",
    "print(f\"Model uploaded to --- > {model_name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800caa83-69d8-433b-980a-23cd2e6cb50b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. 开始微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052d461-6f2b-497e-83cc-d90517465e4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 准备微调代码\n",
    "我们主要使用ChatGLM2提供的P-tuning的代码，做一点修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87c80278-69f6-42c8-a15e-22e64d240b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM2-6B'...\n",
      "Note: switching to '1679b014c6d08005174a215c86190d672b029501'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 1679b01 Merge branch 'main' of github.com:THUDM/ChatGLM2-6B\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "rm -rf ChatGLM2-6B\n",
    "git clone https://github.com/THUDM/ChatGLM2-6B.git\n",
    "cd ChatGLM2-6B\n",
    "git checkout 1679b014c6d08005174a215c86190d672b029501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21105576-92e1-4ca6-895d-bf22b3fbc1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM2-6B/ptuning/sm_ptune_train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM2-6B/ptuning/sm_ptune_train.sh\n",
    "\n",
    "PRE_SEQ_LEN=128 && LR=2e-2 && CUDA_VISIBLE_DEVICES=0 python3 main.py \\\n",
    "    --do_train \\\n",
    "    --train_file $TRAIN_DATASET \\\n",
    "    --validation_file $TEST_DATASET \\\n",
    "    --prompt_column ${PROMPT_COLUMN} \\\n",
    "    --response_column ${RESPONSE_COLUMN}  \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n",
    "    --output_dir ${OUTPUT_DIR} \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 64 \\\n",
    "    --max_target_length 64 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps ${TRAIN_STEPS} \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps ${TRAIN_STEPS} \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4ef9be6-cc81-4826-91be-db92380b1d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM2-6B/ptuning/sm_ptune_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM2-6B/ptuning/sm_ptune_train.py\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = str(os.environ['PYTORCH_CUDA_ALLOC_CONF'])\n",
    "    os.environ['LD_LIBRARY_PATH'] = str(os.environ['LD_LIBRARY_PATH'])\n",
    "    os.environ['TRAIN_DATASET'] = str(os.environ['TRAIN_DATASET'])\n",
    "    os.environ['TEST_DATASET'] = str(os.environ['TEST_DATASET'])\n",
    "    os.environ['PROMPT_COLUMN'] = str(os.environ['PROMPT_COLUMN'])\n",
    "    os.environ['RESPONSE_COLUMN'] = str(os.environ['RESPONSE_COLUMN'])\n",
    "    os.environ['MODEL_NAME_OR_PATH'] = str(os.environ['MODEL_NAME_OR_PATH'])\n",
    "    os.environ['OUTPUT_DIR'] = str(os.environ['OUTPUT_DIR'])\n",
    "    os.environ['MODEL_OUTPUT_S3_PATH'] = str(os.environ['MODEL_OUTPUT_S3_PATH'])\n",
    "\n",
    "    # os.system(\"chmod +x ./s5cmd\")\n",
    "    os.system(\"/bin/bash sm_ptune_train.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f848ff4-6d25-4d7c-be52-fe045aa9abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc67f35a-28d4-4f38-bb65-196b188feda4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChatGLM2-6B/ptuning/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChatGLM2-6B/ptuning/requirements.txt\n",
    "\n",
    "protobuf\n",
    "#git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "transformers==4.28.0\n",
    "cpm_kernels\n",
    "torch>=1.10\n",
    "gradio\n",
    "mdtex2html\n",
    "sentencepiece\n",
    "accelerate\n",
    "datasets\n",
    "huggingface\n",
    "jieba\n",
    "rouge_chinese\n",
    "nltk\n",
    "deepspeed==0.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a8e27b9-97f1-48f7-bd89-4628177cfb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod +x s5cmd\n",
    "! cp s5cmd ChatGLM2-6B/ptuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404b664-ec18-4985-9792-95607e8a147a",
   "metadata": {},
   "source": [
    "## 3.2 定义微调参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eea75cc1-c337-4ac6-9f93-e3d797acabb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Training Job Name\n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = f'chatglm2-finetune-ptuning-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "instance_type  = \"ml.g5.2xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "# 基础模型存放地址 model_name_or_path 在上面上传的时候赋值\n",
    "# s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b\n",
    "# model_name_or_path = 's3://{}/mt_models_uploaded/THUDM--chatglm2-6b/'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 微调模型输出地址\n",
    "output_dir         = '/opt/ml/model/med-qa-chatglm2-6b-ft'\n",
    "model_s3_path      = 's3://{}/mt_models_uploaded/THUDM--chatglm2-6b-finetune/'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 模型环境变量设置\n",
    "environment = {\n",
    "    'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
    "    'TRAIN_DATASET'          : '/opt/ml/input/data/med_qa/med_qa_train.json',\n",
    "    'TEST_DATASET'           : '/opt/ml/input/data/med_qa/med_qa_test.json',\n",
    "    'PROMPT_COLUMN'          : 'question',\n",
    "    'RESPONSE_COLUMN'        : 'answer',\n",
    "    'MODEL_NAME_OR_PATH'     : model_name_or_path,\n",
    "    'OUTPUT_DIR'             : output_dir,\n",
    "    'MODEL_OUTPUT_S3_PATH'   : model_s3_path,\n",
    "    'TRAIN_STEPS'            : '50'\n",
    "}\n",
    "\n",
    "# 数据位置 s3://sagemaker-us-east-1-568765279027/llm/datasets/chatglm/med_qa/med_qa_train.json\n",
    "inputs = {\n",
    "   'med_qa': f\"s3://{sagemaker_default_bucket}/llm/datasets/chatglm2/med_qa/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c85f6af-b9ae-4b4f-875d-9def9c676561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32',\n",
       " 'TRAIN_DATASET': '/opt/ml/input/data/med_qa/med_qa_train.json',\n",
       " 'TEST_DATASET': '/opt/ml/input/data/med_qa/med_qa_test.json',\n",
       " 'PROMPT_COLUMN': 'question',\n",
       " 'RESPONSE_COLUMN': 'answer',\n",
       " 'MODEL_NAME_OR_PATH': 's3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b',\n",
       " 'OUTPUT_DIR': '/opt/ml/model/med-qa-chatglm2-6b-ft',\n",
       " 'MODEL_OUTPUT_S3_PATH': 's3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/',\n",
       " 'TRAIN_STEPS': '50'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfe385-35fa-4b47-8a79-3984cf4e51f4",
   "metadata": {},
   "source": [
    "## 3.3 启动微调训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "477cc154-af4c-480d-8f8a-d69fe3eca03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'sm_ptune_train.py',\n",
    "    source_dir           = './ChatGLM2-6B/ptuning',\n",
    "    instance_type        = instance_type,\n",
    "    instance_count       = instance_count,\n",
    "    base_job_name        = job_name,\n",
    "    role                 = role,\n",
    "    script_mode          = True,\n",
    "    transformers_version = '4.26',\n",
    "    pytorch_version      = '1.13',\n",
    "    py_version           = 'py39',\n",
    "    environment          = environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a2e0b9d1-7366-481e-a72e-b75f497221bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 17:03:16 Starting - Starting the training job...\n",
      "2023-07-19 17:03:34 Starting - Preparing the instances for training......\n",
      "2023-07-19 17:04:38 Downloading - Downloading input data...\n",
      "2023-07-19 17:05:13 Training - Downloading the training image...............\n",
      "2023-07-19 17:07:29 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:00,689 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:00,703 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:00,712 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:00,714 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:01,144 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 51.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cpm_kernels\u001b[0m\n",
      "\u001b[34mDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 416.6/416.6 kB 57.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.37.0-py3-none-any.whl (19.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/19.8 MB 43.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdtex2html\u001b[0m\n",
      "\u001b[34mDownloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface\u001b[0m\n",
      "\u001b[34mDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 51.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting rouge_chinese\u001b[0m\n",
      "\u001b[34mDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 56.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.9.2\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.9.2.tar.gz (779 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 779.3/779.3 kB 46.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.0->-r requirements.txt (line 4)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 16)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 16)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 16)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 16)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 16)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10->-r requirements.txt (line 6)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.100.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.1-py3-none-any.whl (471 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.5/471.5 kB 50.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 13.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.1.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 12.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 28.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 37.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.2.10\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.0/289.0 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->-r requirements.txt (line 7)) (3.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown in /opt/conda/lib/python3.9/site-packages (from mdtex2html->-r requirements.txt (line 8)) (3.4.1)\u001b[0m\n",
      "\u001b[34mCollecting latex2mathml\u001b[0m\n",
      "\u001b[34mDownloading latex2mathml-3.76.0-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 11)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from rouge_chinese->-r requirements.txt (line 14)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 15)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 15)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 7)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 7)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 7)) (4.17.3)\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 7)) (3.0.9)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.5/46.5 kB 12.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 9.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting matplotlib~=3.0\u001b[0m\n",
      "\u001b[34mDownloading matplotlib-3.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 62.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of markupsafe to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting markupsafe~=2.0\u001b[0m\n",
      "\u001b[34mDownloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of markdown-it-py[linkify] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 26.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 7)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.0->-r requirements.txt (line 4)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 21.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.3-py3-none-any.whl (74 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 22.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown->mdtex2html->-r requirements.txt (line 8)) (4.13.0)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->-r requirements.txt (line 8)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 7)) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 7)) (1.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, jieba, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811216 sha256=638e2f3123dcf1de7cdd75dd01c14d2fb7f9dd5112fc906fa1b5d239e5edc919\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7a/86/f9/5b6341574584972377c6d55ff5e8fa53c956a39d63a849ffb4\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=33934ef3eda6aa384c9d9cbb3551bee33020375213269b643927867c53e6b0d6\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=bc1bae95fb3a641a735c95eed2449cef421e2f936c21ec3ead8fa56fb31a6d9d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed jieba ffmpy\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, jieba, huggingface, ffmpy, cpm_kernels, websockets, uc-micro-py, typing-extensions, sniffio, semantic-version, rouge_chinese, python-multipart, orjson, nltk, mdurl, latex2mathml, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, anyio, transformers, starlette, mdtex2html, mdit-py-plugins, httpcore, deepspeed, altair, httpx, fastapi, gradio-client, gradio\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.1.0 altair-5.0.1 anyio-3.7.1 cpm_kernels-1.0.11 deepspeed-0.9.2 fastapi-0.100.0 ffmpy-0.3.1 gradio-3.37.0 gradio-client-0.2.10 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-0.0.1 huggingface-hub-0.16.4 jieba-0.42.1 latex2mathml-3.76.0 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 mdurl-0.1.2 nltk-3.8.1 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 rouge_chinese-1.0.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 transformers-4.28.0 typing-extensions-4.7.1 uc-micro-py-1.0.2 uvicorn-0.23.1 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,352 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,352 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,369 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,395 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,423 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:30,433 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"med_qa\": \"/opt/ml/input/data/med_qa\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"med_qa\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-568765279027/chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_ptune_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_ptune_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_ptune_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"med_qa\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"med_qa\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_ptune_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-568765279027/chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"med_qa\":\"/opt/ml/input/data/med_qa\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"med_qa\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-568765279027/chatglm2-finetune-ptuning-2023-07-19-17-2023-07-19-17-03-13-573/source/sourcedir.tar.gz\",\"module_name\":\"sm_ptune_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_ptune_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MED_QA=/opt/ml/input/data/med_qa\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 sm_ptune_train.py\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:08:34.754: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:34,758 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:08:34,776 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m07/19/2023 17:08:40 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m07/19/2023 17:08:40 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgeneration_config=None,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=4,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=0.02,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/med-qa-chatglm2-6b-ft/runs/Jul19_17-08-40_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=50,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/med-qa-chatglm2-6b-ft,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=True,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model/med-qa-chatglm2-6b-ft,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=50,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m07/19/2023 17:08:40 - WARNING - datasets.builder - Using custom data configuration default-7bac627210c9948e\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7bac627210c9948e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 12787.51it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2126.93it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 111930 examples [00:00, 997502.25 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7bac627210c9948e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 513.63it/s]\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/config.json /tmp/orignal/THUDM--chatglm2-6b/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/quantization.py /tmp/orignal/THUDM--chatglm2-6b/quantization.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/modeling_chatglm.py /tmp/orignal/THUDM--chatglm2-6b/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/tokenization_chatglm.py /tmp/orignal/THUDM--chatglm2-6b/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model.bin.index.json /tmp/orignal/THUDM--chatglm2-6b/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/configuration_chatglm.py /tmp/orignal/THUDM--chatglm2-6b/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/tokenizer.model /tmp/orignal/THUDM--chatglm2-6b/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/tokenizer_config.json /tmp/orignal/THUDM--chatglm2-6b/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00007-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00007-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00004-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00004-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00001-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00001-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00006-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00006-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00003-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00003-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00002-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00002-of-00007.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b/pytorch_model-00005-of-00007.bin /tmp/orignal/THUDM--chatglm2-6b/pytorch_model-00005-of-00007.bin\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-07-19 17:09:30,506 >> loading configuration file /tmp/orignal/THUDM--chatglm2-6b/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-07-19 17:09:30,506 >> loading configuration file /tmp/orignal/THUDM--chatglm2-6b/config.json\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-07-19 17:09:30,506 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|configuration_auto.py:925] 2023-07-19 17:09:30,506 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-07-19 17:09:30,510 >> loading configuration file /tmp/orignal/THUDM--chatglm2-6b/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2023-07-19 17:09:30,510 >> loading configuration file /tmp/orignal/THUDM--chatglm2-6b/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-07-19 17:09:30,511 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"/tmp/orignal/THUDM--chatglm2-6b/\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 32768,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2023-07-19 17:09:30,511 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"/tmp/orignal/THUDM--chatglm2-6b/\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 32768,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-07-19 17:09:30,511 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_auto.py:675] 2023-07-19 17:09:30,511 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2023-07-19 17:09:30,515 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-07-19 17:09:30,540 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[WARNING|auto_factory.py:456] 2023-07-19 17:09:30,540 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-07-19 17:09:30,558 >> loading weights file /tmp/orignal/THUDM--chatglm2-6b/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2023-07-19 17:09:30,558 >> loading weights file /tmp/orignal/THUDM--chatglm2-6b/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-07-19 17:09:30,558 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2023-07-19 17:09:30,558 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-07-19 17:09:41,864 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2023-07-19 17:09:41,864 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-07-19 17:09:41,864 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /tmp/orignal/THUDM--chatglm2-6b/ and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:3192] 2023-07-19 17:09:41,864 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /tmp/orignal/THUDM--chatglm2-6b/ and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-07-19 17:09:41,867 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2023-07-19 17:09:41,867 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/185 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 1/185 [00:00<01:36,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   1%|          | 2/185 [00:01<01:36,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 3/185 [00:01<01:36,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   2%|▏         | 4/185 [00:02<01:34,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 5/185 [00:02<01:33,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   3%|▎         | 6/185 [00:03<01:33,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 7/185 [00:03<01:33,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   4%|▍         | 8/185 [00:04<01:33,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▍         | 9/185 [00:04<01:33,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   5%|▌         | 10/185 [00:05<01:32,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▌         | 11/185 [00:05<01:30,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   6%|▋         | 12/185 [00:06<01:30,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   7%|▋         | 13/185 [00:06<01:29,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 14/185 [00:07<01:29,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   8%|▊         | 15/185 [00:07<01:29,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▊         | 16/185 [00:08<01:27,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   9%|▉         | 17/185 [00:08<01:27,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|▉         | 18/185 [00:09<01:27,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  10%|█         | 19/185 [00:09<01:27,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█         | 20/185 [00:10<01:25,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  11%|█▏        | 21/185 [00:10<01:25,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 22/185 [00:11<01:24,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  12%|█▏        | 23/185 [00:12<01:24,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  13%|█▎        | 24/185 [00:12<01:23,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▎        | 25/185 [00:13<01:24,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  14%|█▍        | 26/185 [00:13<01:23,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▍        | 27/185 [00:14<01:22,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  15%|█▌        | 28/185 [00:14<01:21,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 29/185 [00:15<01:21,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  16%|█▌        | 30/185 [00:15<01:20,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 31/185 [00:16<01:21,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  17%|█▋        | 32/185 [00:16<01:22,  1.86ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 33/185 [00:17<01:21,  1.86ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  18%|█▊        | 34/185 [00:17<01:20,  1.88ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 35/185 [00:18<01:19,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  19%|█▉        | 36/185 [00:18<01:18,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  20%|██        | 37/185 [00:19<01:18,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 38/185 [00:19<01:18,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  21%|██        | 39/185 [00:20<01:19,  1.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 40/185 [00:21<01:19,  1.83ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  22%|██▏       | 41/185 [00:21<01:18,  1.83ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 42/185 [00:22<01:18,  1.83ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  23%|██▎       | 43/185 [00:22<01:16,  1.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 44/185 [00:23<01:14,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  24%|██▍       | 45/185 [00:23<01:13,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▍       | 46/185 [00:24<01:12,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|██▌       | 47/185 [00:24<01:11,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▌       | 48/185 [00:25<01:10,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  26%|██▋       | 49/185 [00:25<01:10,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  27%|██▋       | 50/185 [00:26<01:09,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 51/185 [00:26<01:09,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  28%|██▊       | 52/185 [00:27<01:09,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▊       | 53/185 [00:27<01:08,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  29%|██▉       | 54/185 [00:28<01:07,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|██▉       | 55/185 [00:28<01:07,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  30%|███       | 56/185 [00:29<01:06,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███       | 57/185 [00:29<01:06,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  31%|███▏      | 58/185 [00:30<01:05,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 59/185 [00:30<01:05,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  32%|███▏      | 60/185 [00:31<01:05,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  33%|███▎      | 61/185 [00:32<01:04,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▎      | 62/185 [00:32<01:03,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  34%|███▍      | 63/185 [00:33<01:03,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▍      | 64/185 [00:33<01:03,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  35%|███▌      | 65/185 [00:34<01:02,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 66/185 [00:34<01:01,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  36%|███▌      | 67/185 [00:35<01:01,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 68/185 [00:35<01:00,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  37%|███▋      | 69/185 [00:36<01:00,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 70/185 [00:36<01:00,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  38%|███▊      | 71/185 [00:37<00:59,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 72/185 [00:37<00:58,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  39%|███▉      | 73/185 [00:38<00:57,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  40%|████      | 74/185 [00:38<00:57,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 75/185 [00:39<00:57,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  41%|████      | 76/185 [00:39<00:57,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 77/185 [00:40<00:55,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  42%|████▏     | 78/185 [00:40<00:55,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 79/185 [00:41<00:55,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  43%|████▎     | 80/185 [00:41<00:54,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 81/185 [00:42<00:53,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  44%|████▍     | 82/185 [00:42<00:53,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▍     | 83/185 [00:43<00:52,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  45%|████▌     | 84/185 [00:43<00:52,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▌     | 85/185 [00:44<00:52,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  46%|████▋     | 86/185 [00:45<00:51,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  47%|████▋     | 87/185 [00:45<00:50,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 88/185 [00:46<00:49,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  48%|████▊     | 89/185 [00:46<00:49,  1.96ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▊     | 90/185 [00:47<00:48,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  49%|████▉     | 91/185 [00:47<00:48,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|████▉     | 92/185 [00:48<00:48,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  50%|█████     | 93/185 [00:48<00:47,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████     | 94/185 [00:49<00:47,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  51%|█████▏    | 95/185 [00:49<00:47,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 96/185 [00:50<00:46,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  52%|█████▏    | 97/185 [00:50<00:45,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  53%|█████▎    | 98/185 [00:51<00:45,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▎    | 99/185 [00:51<00:45,  1.88ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  54%|█████▍    | 100/185 [00:52<00:45,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▍    | 101/185 [00:52<00:45,  1.86ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  55%|█████▌    | 102/185 [00:53<00:44,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 103/185 [00:53<00:43,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  56%|█████▌    | 104/185 [00:54<00:43,  1.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 105/185 [00:55<00:43,  1.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  57%|█████▋    | 106/185 [00:55<00:42,  1.88ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 107/185 [00:56<00:41,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  58%|█████▊    | 108/185 [00:56<00:40,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 109/185 [00:57<00:39,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  59%|█████▉    | 110/185 [00:57<00:38,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  60%|██████    | 111/185 [00:58<00:38,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 112/185 [00:58<00:37,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  61%|██████    | 113/185 [00:59<00:36,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 114/185 [00:59<00:36,  1.96ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  62%|██████▏   | 115/185 [01:00<00:36,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 116/185 [01:00<00:35,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  63%|██████▎   | 117/185 [01:01<00:36,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 118/185 [01:01<00:34,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  64%|██████▍   | 119/185 [01:02<00:34,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▍   | 120/185 [01:02<00:33,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  65%|██████▌   | 121/185 [01:03<00:33,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▌   | 122/185 [01:03<00:32,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  66%|██████▋   | 123/185 [01:04<00:32,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  67%|██████▋   | 124/185 [01:04<00:31,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 125/185 [01:05<00:31,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  68%|██████▊   | 126/185 [01:05<00:30,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▊   | 127/185 [01:06<00:29,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  69%|██████▉   | 128/185 [01:06<00:29,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|██████▉   | 129/185 [01:07<00:29,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  70%|███████   | 130/185 [01:08<00:29,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████   | 131/185 [01:08<00:28,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  71%|███████▏  | 132/185 [01:09<00:28,  1.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 133/185 [01:09<00:27,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  72%|███████▏  | 134/185 [01:10<00:26,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  73%|███████▎  | 135/185 [01:10<00:26,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▎  | 136/185 [01:11<00:25,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  74%|███████▍  | 137/185 [01:11<00:24,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▍  | 138/185 [01:12<00:24,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|███████▌  | 139/185 [01:12<00:23,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 140/185 [01:13<00:23,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  76%|███████▌  | 141/185 [01:13<00:22,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 142/185 [01:14<00:22,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  77%|███████▋  | 143/185 [01:14<00:21,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 144/185 [01:15<00:21,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  78%|███████▊  | 145/185 [01:15<00:21,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 146/185 [01:16<00:20,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  79%|███████▉  | 147/185 [01:16<00:19,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  80%|████████  | 148/185 [01:17<00:19,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 149/185 [01:17<00:18,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  81%|████████  | 150/185 [01:18<00:18,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 151/185 [01:18<00:17,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  82%|████████▏ | 152/185 [01:19<00:17,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 153/185 [01:20<00:16,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  83%|████████▎ | 154/185 [01:20<00:16,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 155/185 [01:21<00:16,  1.84ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  84%|████████▍ | 156/185 [01:21<00:15,  1.86ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▍ | 157/185 [01:22<00:14,  1.88ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  85%|████████▌ | 158/185 [01:22<00:14,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▌ | 159/185 [01:23<00:13,  1.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  86%|████████▋ | 160/185 [01:23<00:13,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  87%|████████▋ | 161/185 [01:24<00:12,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 162/185 [01:24<00:12,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  88%|████████▊ | 163/185 [01:25<00:11,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▊ | 164/185 [01:25<00:11,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  89%|████████▉ | 165/185 [01:26<00:10,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|████████▉ | 166/185 [01:26<00:09,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  90%|█████████ | 167/185 [01:27<00:09,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████ | 168/185 [01:27<00:08,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  91%|█████████▏| 169/185 [01:28<00:08,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 170/185 [01:28<00:07,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  92%|█████████▏| 171/185 [01:29<00:07,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  93%|█████████▎| 172/185 [01:30<00:06,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▎| 173/185 [01:30<00:06,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  94%|█████████▍| 174/185 [01:31<00:05,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▍| 175/185 [01:31<00:05,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  95%|█████████▌| 176/185 [01:32<00:04,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 177/185 [01:32<00:04,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  96%|█████████▌| 178/185 [01:33<00:03,  1.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 179/185 [01:33<00:03,  1.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  97%|█████████▋| 180/185 [01:34<00:02,  1.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 181/185 [01:34<00:02,  1.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  98%|█████████▊| 182/185 [01:35<00:01,  1.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 183/185 [01:35<00:01,  1.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  99%|█████████▉| 184/185 [01:36<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 185/185 [01:36<00:00,  2.05ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 185/185 [01:36<00:00,  1.91ba/s]\u001b[0m\n",
      "\u001b[34minput_ids [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 52467, 54725, 30931, 55026, 54570, 33156, 32677, 55398, 31514, 54728, 36347, 32066, 32792, 31514, 52467, 54725, 30931, 55026, 54570, 33156, 32677, 55398, 31514, 54728, 36347, 32066, 33174, 54937, 54798, 39678, 31155, 54716, 35385, 36295, 31123, 35451, 31155, 13, 13, 55437, 31211, 347, 55026, 32180, 40516, 55336, 32289, 32066, 32792, 31702, 31828, 33113, 32709, 32289, 32066, 48968, 36347, 40734, 32066, 32024, 36347, 54865, 32274, 32066, 32024, 54929, 32705, 54554, 54534, 32066, 32544, 31843, 31632, 42360, 32066, 32544, 54530, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\u001b[0m\n",
      "\u001b[34minputs [Round 1]\u001b[0m\n",
      "\u001b[34m问：不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\u001b[0m\n",
      "\u001b[34m答： B超属于超声波经常检查是不好的而且也没有必要经常检查的一般怀孕两个月检查一下怀孕五个月检查一下快出生时在检查就可以还有就是不舒服检查就可以的\u001b[0m\n",
      "\u001b[34mlabel_ids\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 347, 55026, 32180, 40516, 55336, 32289, 32066, 32792, 31702, 31828, 33113, 32709, 32289, 32066, 48968, 36347, 40734, 32066, 32024, 36347, 54865, 32274, 32066, 32024, 54929, 32705, 54554, 54534, 32066, 32544, 31843, 31632, 42360, 32066, 32544, 54530, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\u001b[0m\n",
      "\u001b[34mlabels B超属于超声波经常检查是不好的而且也没有必要经常检查的一般怀孕两个月检查一下怀孕五个月检查一下快出生时在检查就可以还有就是不舒服检查就可以的\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:564] 2023-07-19 17:11:21,625 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:564] 2023-07-19 17:11:21,625 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2023-07-19 17:11:21,637 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2023-07-19 17:11:21,637 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2023-07-19 17:11:21,637 >>   Num examples = 184,739\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2023-07-19 17:11:21,637 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2023-07-19 17:11:21,637 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2023-07-19 17:11:21,637 >>   Num examples = 184,739\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2023-07-19 17:11:21,637 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2023-07-19 17:11:21,637 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2023-07-19 17:11:21,637 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2023-07-19 17:11:21,637 >>   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2023-07-19 17:11:21,637 >>   Total optimization steps = 50\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2023-07-19 17:11:21,637 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2023-07-19 17:11:21,637 >>   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2023-07-19 17:11:21,637 >>   Total optimization steps = 50\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2023-07-19 17:11:21,638 >>   Number of trainable parameters = 1,835,008\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2023-07-19 17:11:21,638 >>   Number of trainable parameters = 1,835,008\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.778: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.805 algo-1:164 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.838 algo-1:164 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.838 algo-1:164 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.839 algo-1:164 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.839 algo-1:164 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-07-19 17:11:21.839 algo-1:164 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m07/19/2023 17:11:22 - WARNING - transformers_modules.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/50 [00:04<03:16,  4.01s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:06<02:15,  2.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:08<01:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:10<01:44,  2.27s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:12<01:37,  2.17s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:14<01:33,  2.12s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:16<01:29,  2.08s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:18<01:26,  2.05s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:20<01:23,  2.04s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:22<01:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.988, 'learning_rate': 0.016, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:22<01:21,  2.03s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [00:24<01:18,  2.02s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [00:26<01:16,  2.01s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [00:28<01:14,  2.01s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [00:30<01:12,  2.01s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [00:32<01:10,  2.00s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [00:34<01:08,  2.00s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [00:36<01:06,  2.00s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [00:38<01:04,  2.00s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [00:40<01:02,  2.00s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:42<01:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.5487, 'learning_rate': 0.012, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [00:42<01:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [00:44<00:58,  2.00s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [00:46<00:56,  2.00s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [00:48<00:54,  2.00s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [00:50<00:52,  2.00s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [00:52<00:49,  2.00s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [00:54<00:47,  2.00s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [00:56<00:45,  2.00s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [00:58<00:43,  2.00s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [01:00<00:41,  2.00s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [01:02<00:39,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.3817, 'learning_rate': 0.008, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [01:02<00:39,  2.00s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [01:04<00:37,  2.00s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [01:06<00:35,  2.00s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [01:08<00:33,  2.00s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [01:10<00:31,  2.00s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [01:12<00:30,  2.00s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [01:14<00:28,  2.00s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [01:16<00:26,  2.00s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [01:18<00:24,  2.00s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [01:20<00:22,  2.00s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [01:22<00:19,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.4257, 'learning_rate': 0.004, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [01:22<00:19,  2.00s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [01:24<00:17,  2.00s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [01:26<00:15,  2.00s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [01:28<00:13,  2.00s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [01:30<00:11,  2.00s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [01:31<00:09,  2.00s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [01:33<00:07,  2.00s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [01:35<00:05,  2.00s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [01:37<00:03,  2.00s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [01:39<00:01,  2.00s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [01:41<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.3904, 'learning_rate': 0.0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [01:41<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-19 17:13:03,639 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-19 17:13:03,639 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-19 17:13:03,640 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-19 17:13:03,640 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-19 17:13:03,648 >> Model weights saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-19 17:13:03,648 >> Model weights saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,649 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,649 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,649 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,649 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-07-19 17:13:03,670 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-07-19 17:13:03,670 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [01:42<00:00,  2.00s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 102.0319, 'train_samples_per_second': 7.841, 'train_steps_per_second': 0.49, 'train_loss': 3.54689453125, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [01:42<00:00,  2.04s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =        0.0\u001b[0m\n",
      "\u001b[34mtrain_loss               =     3.5469\n",
      "  train_runtime            = 0:01:42.03\n",
      "  train_samples            =     184739\n",
      "  train_samples_per_second =      7.841\n",
      "  train_steps_per_second   =       0.49\u001b[0m\n",
      "\u001b[34m------saving model!-----\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,673 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,673 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,673 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,673 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSaving PrefixEncoder\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-19 17:13:03,679 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-19 17:13:03,679 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-19 17:13:03,679 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-19 17:13:03,679 >> Configuration saved in /opt/ml/model/med-qa-chatglm2-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-19 17:13:03,686 >> Model weights saved in /opt/ml/model/med-qa-chatglm2-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-19 17:13:03,686 >> Model weights saved in /opt/ml/model/med-qa-chatglm2-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,687 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-19 17:13:03,687 >> tokenizer config file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,687 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-19 17:13:03,687 >> Special tokens file saved in /opt/ml/model/med-qa-chatglm2-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34msave_model_dir : /opt/ml/model/med-qa-chatglm2-6b-ft\u001b[0m\n",
      "\u001b[34m------model is saved!-----\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/special_tokens_map.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/trainer_state.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/generation_config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/all_results.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/all_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/generation_config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/trainer_state.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/special_tokens_map.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer_config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/train_results.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/train_results.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/configuration_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/training_args.bin s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/scheduler.pt s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/scheduler.pt\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer_config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/modeling_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/modeling_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/modeling_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/config.json s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/config.json\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/configuration_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/configuration_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/training_args.bin s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/quantization.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/tokenization_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/rng_state.pth s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/rng_state.pth\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/tokenization_chatglm.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/tokenization_chatglm.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/quantization.py s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/quantization.py\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer.model s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/tokenizer.model s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/pytorch_model.bin s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/pytorch_model.bin s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp /opt/ml/model/med-qa-chatglm2-6b-ft/checkpoint-50/optimizer.pt s3://sagemaker-us-east-1-568765279027/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/optimizer.pt\u001b[0m\n",
      "\u001b[34m2023-07-19 17:13:05,036 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:13:05,036 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-19 17:13:05,037 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-07-19 17:13:42 Uploading - Uploading generated training model\n",
      "2023-07-19 17:13:42 Completed - Training job completed\n",
      "Training seconds: 543\n",
      "Billable seconds: 543\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48c6d6-b465-4cd7-bbc7-3ca3d2951dbb",
   "metadata": {},
   "source": [
    "# 4. 模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b753e-a6b8-4bba-bff7-b094e1fce67c",
   "metadata": {},
   "source": [
    "# 4.1 获取Runtime资源配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d74b206e-6cd0-4b0c-8a91-0d924699f5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a1edc-5090-4570-b3a6-0691b6304f50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.2 准备Dummy模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5da1ff4d-cc76-42fc-a711-8f822aed6a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n"
     ]
    }
   ],
   "source": [
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "!rm -f dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d844ef-4b13-4ba9-add1-26d0c1913459",
   "metadata": {},
   "source": [
    "# 4.3 配置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d2c840-4ed1-4874-84c4-6c0309f3c552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name                  = \"mt-chatglm2-6b-ft-g4dn-model\"\n",
    "entry_point                 = 'inference-chatglm2-ft.py'\n",
    "framework_version           = '1.13.1'\n",
    "py_version                  = 'py39'\n",
    "base_model_name_or_path     = 's3://{}/mt_models_uploaded/THUDM--chatglm2-6b/'.format(sagemaker_default_bucket)\n",
    "finetune_model_name_or_path = 's3://{}/mt_models_uploaded/THUDM--chatglm2-6b-finetune/med-qa-chatglm2-6b-ft/checkpoint-50/pytorch_model.bin'.format(sagemaker_default_bucket)\n",
    "\n",
    "# 模型环境变量设置\n",
    "model_environment  = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': '600',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
    "    'MODEL_NAME_OR_PATH'            : base_model_name_or_path,\n",
    "    'PRE_SEQ_LEN'                   : '128',\n",
    "    'FINETUNE_MODEL_NAME_OR_PATH'   : finetune_model_name_or_path,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076681b6-3064-4dd5-b9ca-83cedab9da2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name              = model_name,\n",
    "    model_data        = \"./model.tar.gz\",\n",
    "    entry_point       = entry_point,\n",
    "    source_dir        = './code-chatglm2',\n",
    "    role              = role,\n",
    "    framework_version = framework_version, \n",
    "    py_version        = py_version,\n",
    "    env               = model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13077f97-3549-46bf-b175-aee0aa050c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod +x s5cmd\n",
    "! cp s5cmd code-chatglm2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf89bb-5d55-4871-b339-506742d4c4a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.4 部署微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92557cf7-950a-46ca-b58c-2e239bb02207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name         = 'mt-chatglm2-6b-ft-g4dn'\n",
    "instance_type         = 'ml.g4dn.2xlarge'\n",
    "instance_count        = 1\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name          = endpoint_name,\n",
    "    instance_type          = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer             = JSONSerializer(),\n",
    "    deserializer           = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3c7af-c214-4db1-b2f7-0973f1f5eaa8",
   "metadata": {},
   "source": [
    "# 4.5 测试微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202bd61-2565-4ff8-b79a-811742391218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"ask\": \"为什么会得腰间盘突出？\"\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd08ecc-88c8-4c7d-ab23-e2c51804827d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"ask\": \"天为什么是蓝的？\"\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2005c59-3883-4d94-82b7-813d48229b23",
   "metadata": {},
   "source": [
    "## 使用原先的ChatGLM并且没有知识库的方式进行问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7700f6fc-6d22-4aa5-b7ca-d41a9e63af65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腰间盘突出是一种退行性疾病，主要发生在年龄较大的人群。腰部肌肉和韧带的劳损是腰间盘突出的主要原因。那么，如何防治腰间盘突出呢？\\n\\n首先，要避免长时间保持一个姿势，如长时间坐着或站着，这会增加腰部肌肉和韧带的劳损，加重腰间盘突出的风险。\\n\\n其次，加强腰部肌肉的锻炼，有助于缓解腰部肌肉和韧带的劳损，减轻腰间盘突出的症状。例如，可以做一些腰部伸展运动，如仰卧起坐、俯卧起坐、仰卧举腿等。\\n\\n此外，保持适当的体重，避免过度肥胖，也能减轻腰部肌肉和韧带的负担，降低腰间盘突出的风险。\\n\\n在治疗腰间盘突出方面，可以采用药物治疗、物理治疗、手术治疗等方法。不过，治疗腰间盘突出需要根据医生的建议进行，并积极配合治疗，以提高治疗效果。'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "endpoint_name = 'mt-chatglm2-6b-g4dn'\n",
    "\n",
    "def query_endpoint(encoded_json):\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    generated_text = model_predictions[\"answer\"]\n",
    "    return generated_text\n",
    "\n",
    "payload = {\"ask\": \"腰间盘突出怎么防治？\", \"parameters\": {}, \"history\": []}\n",
    "query_endpoint(json.dumps(payload).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c28bbdc6-1f23-42ef-aeb3-29e098f1fbac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腰间盘突出通常需要药物治疗,但具体治疗方式还需要根据病情的具体情况而定。以下是一些可能有助于缓解腰间盘突出的症状的方法:\\n\\n1. 休息:减轻腰部压力,减少症状。建议在症状最严重时休息数天。\\n\\n2. 物理疗法:物理疗法包括按摩、针灸、理疗等,可以缓解疼痛和减轻炎症。\\n\\n3. 药物治疗:药物治疗包括非甾体抗炎药、镇痛剂、肌肉松弛剂等,可以缓解疼痛和减轻炎症。不过,药物治疗应在医生的指导下进行,以避免不良反应。\\n\\n4. 运动疗法:适当的运动可以增强腰部肌肉力量和灵活性,缓解疼痛和减轻症状。不过,运动疗法应在医生的指导下进行,以避免进一步损伤。\\n\\n腰间盘突出需要综合治疗,包括药物治疗、物理疗法、运动疗法等。在医生的指导下进行治疗,并遵守医生的治疗方案,是缓解腰间盘突出的症状的有效途径。'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "endpoint_name = 'mt-chatglm2-6b-ds'\n",
    "\n",
    "def query_endpoint(encoded_json):\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    generated_text = model_predictions[\"outputs\"]\n",
    "    return generated_text\n",
    "payload = {\"inputs\": \"腰间盘突出可以不吃药吗？\", \"parameters\": {}, \"history\": []}\n",
    "query_endpoint(json.dumps(payload).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd114c-e728-4070-b7da-83261c8ed051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0586b2e7-1e42-4a10-8626-f3b97bd03cc0",
   "metadata": {},
   "source": [
    "# 4.6 清除资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "116f8f17-d43a-464b-99f4-a745b9f8841b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: pytorch-inference-2023-07-19-17-32-59-456\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: mt-chatglm2-6b-ft-g4dn\n",
      "INFO:sagemaker:Deleting endpoint with name: mt-chatglm2-6b-ft-g4dn\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788e868-12f6-4daa-9907-e0f3b3ae6eb1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (mt_python3)",
   "language": "python",
   "name": "mt_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
